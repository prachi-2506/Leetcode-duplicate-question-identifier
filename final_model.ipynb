{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41ec1e4-4979-44b8-a9a6-3bb3228a31b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape: (3000, 18)\n",
      "\n",
      "Sample cleaned data:\n",
      "                                               title  \\\n",
      "0                                         1. Two Sum   \n",
      "1                                 2. Add Two Numbers   \n",
      "2  3. Longest Substring Without Repeating Characters   \n",
      "3                     4. Median of Two Sorted Arrays   \n",
      "4                   5. Longest Palindromic Substring   \n",
      "5                               6. Zigzag Conversion   \n",
      "6                                 7. Reverse Integer   \n",
      "7                        8. String to Integer (atoi)   \n",
      "8                               9. Palindrome Number   \n",
      "9                    10. Regular Expression Matching   \n",
      "\n",
      "                                      clean_title  \\\n",
      "0                                         Two Sum   \n",
      "1                                 Add Two Numbers   \n",
      "2  Longest Substring Without Repeating Characters   \n",
      "3                     Median of Two Sorted Arrays   \n",
      "4                   Longest Palindromic Substring   \n",
      "5                               Zigzag Conversion   \n",
      "6                                 Reverse Integer   \n",
      "7                        String to Integer (atoi)   \n",
      "8                               Palindrome Number   \n",
      "9                     Regular Expression Matching   \n",
      "\n",
      "                                   similar_questions  \\\n",
      "0  [\"'3Sum'\", \"'4Sum'\", \"'Two Sum II - Input Arra...   \n",
      "1  [\"'Multiply Strings'\", \"'Add Binary'\", \"'Sum o...   \n",
      "2  [\"'Longest Substring with At Most Two Distinct...   \n",
      "3           [\"'Median of a Row Wise Sorted Matrix'\"]   \n",
      "4  [\"'Shortest Palindrome'\", \"'Palindrome Permuta...   \n",
      "5                                               ['']   \n",
      "6  [\"'String to Integer (atoi)'\", \"'Reverse Bits'...   \n",
      "7  [\"'Reverse Integer'\", \"'Valid Number'\", \"'Chec...   \n",
      "8  [\"'Palindrome Linked List'\", \"'Find Palindrome...   \n",
      "9                            [\"'Wildcard Matching'\"]   \n",
      "\n",
      "                             clean_similar_questions  \n",
      "0  [3Sum, 4Sum, Two Sum II - Input Array Is Sorte...  \n",
      "1  [Multiply Strings, Add Binary, Sum of Two Inte...  \n",
      "2  [Longest Substring with At Most Two Distinct C...  \n",
      "3               [Median of a Row Wise Sorted Matrix]  \n",
      "4  [Shortest Palindrome, Palindrome Permutation, ...  \n",
      "5                                                 []  \n",
      "6  [String to Integer (atoi), Reverse Bits, A Num...  \n",
      "7  [Reverse Integer, Valid Number, Check if Numbe...  \n",
      "8  [Palindrome Linked List, Find Palindrome With ...  \n",
      "9                                [Wildcard Matching]  \n",
      "\n",
      "Number of missing similar questions: 2\n",
      "Examples of missing titles: ['Pow(x', 'n)']\n",
      "\n",
      "✅ Cleaned dataset saved as 'leetcode_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Import Libraries ---\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "\n",
    "# --- 2. Load Dataset ---\n",
    "# Replace 'leetcode_data.csv' with your file path\n",
    "df = pd.read_csv(\"leetcode.csv\")\n",
    "\n",
    "# --- 3. Display Basic Info ---\n",
    "print(\"Initial Shape:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "# --- 4. Clean the 'title' column ---\n",
    "# Remove any numbering or prefixes like \"1. Two Sum\" → \"Two Sum\"\n",
    "def clean_title(title):\n",
    "    if pd.isna(title):\n",
    "        return \"\"\n",
    "    # Remove leading numbers, dots, spaces\n",
    "    cleaned = re.sub(r\"^\\d+\\.\\s*\", \"\", str(title)).strip()\n",
    "    return cleaned\n",
    "\n",
    "df[\"clean_title\"] = df[\"title\"].apply(clean_title)\n",
    "\n",
    "# --- 5. Clean the 'similar_questions' column ---\n",
    "def clean_similar_questions(x):\n",
    "    if pd.isna(x) or x in ['[]', 'None', '', 'nan']:\n",
    "        return []\n",
    "    try:\n",
    "        # Convert string that looks like a list into a real Python list\n",
    "        lst = ast.literal_eval(x)\n",
    "        # Remove extra quotes and spaces from each element\n",
    "        lst = [s.strip(\" '\\\"\\n\\t\") for s in lst if isinstance(s, str) and s.strip()]\n",
    "        # Clean titles similarly (remove numbering if they exist)\n",
    "        lst = [re.sub(r\"^\\d+\\.\\s*\", \"\", s).strip() for s in lst]\n",
    "        return lst\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "df[\"clean_similar_questions\"] = df[\"similar_questions\"].apply(clean_similar_questions)\n",
    "\n",
    "# --- 6. Verify Cleaning ---\n",
    "print(\"\\nSample cleaned data:\")\n",
    "print(df[[\"title\", \"clean_title\", \"similar_questions\", \"clean_similar_questions\"]].head(10))\n",
    "\n",
    "# --- 7. (Optional) Remove Duplicate or Empty Titles ---\n",
    "df = df[df[\"clean_title\"] != \"\"].drop_duplicates(subset=[\"clean_title\"]).reset_index(drop=True)\n",
    "\n",
    "# --- 8. (Optional) Check for unmatched titles ---\n",
    "# Helps you see which 'similar_questions' don't exist in the main title list\n",
    "all_titles = set(df[\"clean_title\"])\n",
    "missing = set(q for lst in df[\"clean_similar_questions\"] for q in lst if q not in all_titles)\n",
    "\n",
    "print(f\"\\nNumber of missing similar questions: {len(missing)}\")\n",
    "if len(missing) > 0:\n",
    "    print(\"Examples of missing titles:\", list(missing)[:10])\n",
    "\n",
    "# --- 9. Save Cleaned Dataset ---\n",
    "df.to_csv(\"leetcode_cleaned.csv\", index=False)\n",
    "print(\"\\n✅ Cleaned dataset saved as 'leetcode_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91834d47-ba5b-4663-8675-618b956ed7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 210562\n",
      "Epoch 1/10, Loss: 0.0094\n",
      "Epoch 2/10, Loss: 0.0085\n",
      "Epoch 3/10, Loss: 0.0082\n",
      "Epoch 4/10, Loss: 0.0076\n",
      "Epoch 5/10, Loss: 0.0076\n",
      "Epoch 6/10, Loss: 0.0078\n",
      "Epoch 7/10, Loss: 0.0082\n",
      "Epoch 8/10, Loss: 0.0074\n",
      "Epoch 9/10, Loss: 0.0075\n",
      "Epoch 10/10, Loss: 0.0093\n",
      "\n",
      "✅ Validation Accuracy: 56.44%\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Import libraries ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- 2. Load cleaned dataset ---\n",
    "df = pd.read_csv(\"leetcode_cleaned.csv\")\n",
    "\n",
    "# Combine title + problem_description for richer text\n",
    "df[\"full_text\"] = df[\"clean_title\"].fillna('') + \" \" + df[\"problem_description\"].fillna('')\n",
    "\n",
    "# --- 3. Prepare Positive Pairs (duplicates / similar questions) ---\n",
    "pairs = []\n",
    "for _, row in df.iterrows():\n",
    "    main = row[\"clean_title\"]\n",
    "    for sim in row[\"clean_similar_questions\"]:\n",
    "        pairs.append((main, sim, 1))  # Label 1 = duplicate/similar\n",
    "\n",
    "# Negative sampling: random non-duplicates\n",
    "titles = df[\"clean_title\"].tolist()\n",
    "np.random.shuffle(titles)\n",
    "for i in range(len(pairs)):\n",
    "    a = titles[np.random.randint(len(titles))]\n",
    "    b = titles[np.random.randint(len(titles))]\n",
    "    pairs.append((a, b, 0))  # Label 0 = not similar\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=[\"text1\", \"text2\", \"label\"])\n",
    "print(\"Total pairs:\", len(pairs_df))\n",
    "\n",
    "# --- 4. TF-IDF vectorization (build vocabulary from scratch) ---\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "all_texts = df[\"full_text\"].tolist()\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "def text_to_vec(text):\n",
    "    return torch.tensor(vectorizer.transform([text]).toarray(), dtype=torch.float32)\n",
    "\n",
    "# --- 5. Define a simple neural embedding model ---\n",
    "class TextEmbedder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128):\n",
    "        super(TextEmbedder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "embed_dim = 128\n",
    "model = TextEmbedder(input_dim=5000, embed_dim=embed_dim)\n",
    "\n",
    "# --- 6. Contrastive loss function ---\n",
    "def cosine_loss(a, b, label):\n",
    "    cos = nn.functional.cosine_similarity(a, b)\n",
    "    loss = torch.mean((1 - cos) * label + (1 + cos) * (1 - label))\n",
    "    return loss\n",
    "\n",
    "# --- 7. Train/test split ---\n",
    "train_df, val_df = train_test_split(pairs_df, test_size=0.1, random_state=42)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# --- 8. Training loop ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for _, row in train_df.sample(2000).iterrows():  # sample subset for speed\n",
    "        t1 = text_to_vec(row[\"text1\"])\n",
    "        t2 = text_to_vec(row[\"text2\"])\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "\n",
    "        emb1 = model(t1)\n",
    "        emb2 = model(t2)\n",
    "\n",
    "        loss = cosine_loss(emb1, emb2, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_df):.4f}\")\n",
    "\n",
    "# --- 9. Evaluate on validation set ---\n",
    "model.eval()\n",
    "sims, labels = [], []\n",
    "for _, row in val_df.iterrows():\n",
    "    t1 = text_to_vec(row[\"text1\"])\n",
    "    t2 = text_to_vec(row[\"text2\"])\n",
    "    label = row[\"label\"]\n",
    "\n",
    "    emb1 = model(t1)\n",
    "    emb2 = model(t2)\n",
    "\n",
    "    sim = nn.functional.cosine_similarity(emb1, emb2).item()\n",
    "    sims.append(sim)\n",
    "    labels.append(label)\n",
    "\n",
    "# Compute basic accuracy (using threshold)\n",
    "preds = [1 if s > 0.5 else 0 for s in sims]\n",
    "accuracy = np.mean(np.array(preds) == np.array(labels))\n",
    "print(f\"\\n✅ Validation Accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0242f65-17c1-4f8b-b426-383a69569f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model and vectorizer saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "\n",
    "# Save your trained PyTorch model\n",
    "torch.save(model, \"leetcode_model.pt\")\n",
    "\n",
    "# Save your TF-IDF vectorizer (if you used one)\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "print(\"✅ Model and vectorizer saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d77a80e3-b54b-4666-8df1-2d52b3a3e2ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.TextEmbedder was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.TextEmbedder])` or the `torch.serialization.safe_globals([__main__.TextEmbedder])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Then create the model instance and LOAD STATE DICT!\u001b[39;00m\n\u001b[0;32m     21\u001b[0m model \u001b[38;5;241m=\u001b[39m TextEmbedder(input_dim\u001b[38;5;241m=\u001b[39minput_dim, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleetcode_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\Desktop\\Anaconda\\Lib\\site-packages\\torch\\serialization.py:1529\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1521\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1522\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1523\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1526\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1527\u001b[0m                 )\n\u001b[0;32m   1528\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1529\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1531\u001b[0m             opened_zipfile,\n\u001b[0;32m   1532\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1535\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1536\u001b[0m         )\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.TextEmbedder was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.TextEmbedder])` or the `torch.serialization.safe_globals([__main__.TextEmbedder])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextEmbedder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128):\n",
    "        super(TextEmbedder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Load the vectorizer, get input_dim from it\n",
    "import joblib\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "input_dim = vectorizer.transform(['example']).shape[1]\n",
    "\n",
    "# Then create the model instance and LOAD STATE DICT!\n",
    "model = TextEmbedder(input_dim=input_dim, embed_dim=128)\n",
    "model.load_state_dict(torch.load(\"leetcode_model.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d5686-8864-4b2e-a2b3-7da23d925b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
