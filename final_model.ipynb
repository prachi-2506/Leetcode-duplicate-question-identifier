{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52fecbb8-cb13-40df-ae3c-f5c8868520c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape: (3000, 20)\n",
      "\n",
      "Sample cleaned data:\n",
      "                                               title  \\\n",
      "0                                         1. Two Sum   \n",
      "1                                 2. Add Two Numbers   \n",
      "2  3. Longest Substring Without Repeating Characters   \n",
      "3                     4. Median of Two Sorted Arrays   \n",
      "4                   5. Longest Palindromic Substring   \n",
      "5                               6. Zigzag Conversion   \n",
      "6                                 7. Reverse Integer   \n",
      "7                        8. String to Integer (atoi)   \n",
      "8                               9. Palindrome Number   \n",
      "9                    10. Regular Expression Matching   \n",
      "\n",
      "                                      clean_title  \\\n",
      "0                                         Two Sum   \n",
      "1                                 Add Two Numbers   \n",
      "2  Longest Substring Without Repeating Characters   \n",
      "3                     Median of Two Sorted Arrays   \n",
      "4                   Longest Palindromic Substring   \n",
      "5                               Zigzag Conversion   \n",
      "6                                 Reverse Integer   \n",
      "7                        String to Integer (atoi)   \n",
      "8                               Palindrome Number   \n",
      "9                     Regular Expression Matching   \n",
      "\n",
      "                                   similar_questions  \\\n",
      "0  [\"'3Sum'\", \"'4Sum'\", \"'Two Sum II - Input Arra...   \n",
      "1  [\"'Multiply Strings'\", \"'Add Binary'\", \"'Sum o...   \n",
      "2  [\"'Longest Substring with At Most Two Distinct...   \n",
      "3           [\"'Median of a Row Wise Sorted Matrix'\"]   \n",
      "4  [\"'Shortest Palindrome'\", \"'Palindrome Permuta...   \n",
      "5                                               ['']   \n",
      "6  [\"'String to Integer (atoi)'\", \"'Reverse Bits'...   \n",
      "7  [\"'Reverse Integer'\", \"'Valid Number'\", \"'Chec...   \n",
      "8  [\"'Palindrome Linked List'\", \"'Find Palindrome...   \n",
      "9                            [\"'Wildcard Matching'\"]   \n",
      "\n",
      "                             clean_similar_questions  \n",
      "0  [3Sum, 4Sum, Two Sum II - Input Array Is Sorte...  \n",
      "1  [Multiply Strings, Add Binary, Sum of Two Inte...  \n",
      "2  [Longest Substring with At Most Two Distinct C...  \n",
      "3               [Median of a Row Wise Sorted Matrix]  \n",
      "4  [Shortest Palindrome, Palindrome Permutation, ...  \n",
      "5                                                 []  \n",
      "6  [String to Integer (atoi), Reverse Bits, A Num...  \n",
      "7  [Reverse Integer, Valid Number, Check if Numbe...  \n",
      "8  [Palindrome Linked List, Find Palindrome With ...  \n",
      "9                                [Wildcard Matching]  \n",
      "\n",
      "Number of missing similar questions: 2\n",
      "Examples of missing titles: ['n)', 'Pow(x']\n",
      "\n",
      "✅ Cleaned dataset saved as 'leetcode_cleaned.csv'\n",
      "Total pairs: 210562\n",
      "Epoch 1/10, Loss: 0.0096\n",
      "Epoch 2/10, Loss: 0.0086\n",
      "Epoch 3/10, Loss: 0.0081\n",
      "Epoch 4/10, Loss: 0.0079\n",
      "Epoch 5/10, Loss: 0.0077\n",
      "Epoch 6/10, Loss: 0.0075\n",
      "Epoch 7/10, Loss: 0.0074\n",
      "Epoch 8/10, Loss: 0.0072\n",
      "Epoch 9/10, Loss: 0.0072\n",
      "Epoch 10/10, Loss: 0.0072\n",
      "\n",
      "✅ Validation Accuracy: 67.44%\n",
      "✅ Model weights and vectorizer saved successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextEmbedder(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=5000, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 1. Import Libraries ---\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "\n",
    "# --- 2. Load Dataset & Clean Titles ---\n",
    "df = pd.read_csv(\"leetcode.csv\")\n",
    "def clean_title(title):\n",
    "    if pd.isna(title):\n",
    "        return \"\"\n",
    "    cleaned = re.sub(r\"^\\d+\\.\\s*\", \"\", str(title)).strip()\n",
    "    return cleaned\n",
    "df[\"clean_title\"] = df[\"title\"].apply(clean_title)\n",
    "\n",
    "def clean_similar_questions(x):\n",
    "    if pd.isna(x) or x in ['[]', 'None', '', 'nan']:\n",
    "        return []\n",
    "    try:\n",
    "        lst = ast.literal_eval(x)\n",
    "        lst = [s.strip(\" '\\\"\\n\\t\") for s in lst if isinstance(s, str) and s.strip()]\n",
    "        lst = [re.sub(r\"^\\d+\\.\\s*\", \"\", s).strip() for s in lst]\n",
    "        return lst\n",
    "    except Exception:\n",
    "        return []\n",
    "df[\"clean_similar_questions\"] = df[\"similar_questions\"].apply(clean_similar_questions)\n",
    "print(\"Initial Shape:\", df.shape)\n",
    "print(\"\\nSample cleaned data:\")\n",
    "print(df[[\"title\", \"clean_title\", \"similar_questions\", \"clean_similar_questions\"]].head(10))\n",
    "\n",
    "# Remove duplicate or empty titles\n",
    "df = df[df[\"clean_title\"] != \"\"].drop_duplicates(subset=[\"clean_title\"]).reset_index(drop=True)\n",
    "\n",
    "# Optional: report missing similar question titles\n",
    "all_titles = set(df[\"clean_title\"])\n",
    "missing = set(q for lst in df[\"clean_similar_questions\"] for q in lst if q not in all_titles)\n",
    "print(f\"\\nNumber of missing similar questions: {len(missing)}\")\n",
    "if len(missing) > 0:\n",
    "    print(\"Examples of missing titles:\", list(missing)[:10])\n",
    "\n",
    "# --- 3. Save Cleaned Dataset ---\n",
    "df.to_csv(\"leetcode_cleaned.csv\", index=False)\n",
    "print(\"\\n✅ Cleaned dataset saved as 'leetcode_cleaned.csv'\")\n",
    "\n",
    "# --- 4. Vectorization & Pair Preparation ---\n",
    "df = pd.read_csv(\"leetcode_cleaned.csv\")\n",
    "df[\"full_text\"] = df[\"clean_title\"].fillna('') + \" \" + df[\"problem_description\"].fillna('')\n",
    "pairs = []\n",
    "for _, row in df.iterrows():\n",
    "    main = row[\"clean_title\"]\n",
    "    for sim in row[\"clean_similar_questions\"]:\n",
    "        pairs.append((main, sim, 1))  # Similar pair (label 1)\n",
    "\n",
    "titles = df[\"clean_title\"].tolist()\n",
    "np.random.shuffle(titles)\n",
    "for i in range(len(pairs)):\n",
    "    a = titles[np.random.randint(len(titles))]\n",
    "    b = titles[np.random.randint(len(titles))]\n",
    "    pairs.append((a, b, 0))  # Random negative (label 0)\n",
    "\n",
    "pairs_df = pd.DataFrame(pairs, columns=[\"text1\", \"text2\", \"label\"])\n",
    "print(\"Total pairs:\", len(pairs_df))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "all_texts = df[\"full_text\"].tolist()\n",
    "vectorizer.fit(all_texts)\n",
    "def text_to_vec(text):\n",
    "    return torch.tensor(vectorizer.transform([text]).toarray(), dtype=torch.float32)\n",
    "\n",
    "# --- 5. Embedding Model (class definition) ---\n",
    "class TextEmbedder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128):\n",
    "        super(TextEmbedder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "embed_dim = 128\n",
    "model = TextEmbedder(input_dim=5000, embed_dim=embed_dim)\n",
    "\n",
    "# --- 6. Contrastive Loss Function ---\n",
    "def cosine_loss(a, b, label):\n",
    "    cos = nn.functional.cosine_similarity(a, b)\n",
    "    loss = torch.mean((1 - cos) * label + (1 + cos) * (1 - label))\n",
    "    return loss\n",
    "\n",
    "# --- 7. Train/Test Split ---\n",
    "train_df, val_df = train_test_split(pairs_df, test_size=0.1, random_state=42)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "# --- 8. Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for _, row in train_df.sample(2000).iterrows():  # sample subset\n",
    "        t1 = text_to_vec(row[\"text1\"])\n",
    "        t2 = text_to_vec(row[\"text2\"])\n",
    "        label = torch.tensor(row[\"label\"], dtype=torch.float32)\n",
    "        emb1 = model(t1)\n",
    "        emb2 = model(t2)\n",
    "        loss = cosine_loss(emb1, emb2, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_df):.4f}\")\n",
    "\n",
    "# --- 9. Validation ---\n",
    "model.eval()\n",
    "sims, labels = [], []\n",
    "for _, row in val_df.iterrows():\n",
    "    t1 = text_to_vec(row[\"text1\"])\n",
    "    t2 = text_to_vec(row[\"text2\"])\n",
    "    label = row[\"label\"]\n",
    "    emb1 = model(t1)\n",
    "    emb2 = model(t2)\n",
    "    sim = nn.functional.cosine_similarity(emb1, emb2).item()\n",
    "    sims.append(sim)\n",
    "    labels.append(label)\n",
    "preds = [1 if s > 0.5 else 0 for s in sims]\n",
    "accuracy = np.mean(np.array(preds) == np.array(labels))\n",
    "print(f\"\\n✅ Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# --- 10. SAVE: Use only state_dict for deployment! ---\n",
    "torch.save(model.state_dict(), \"leetcode_model.pt\")   # <-- THIS is the critical change!\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "print(\"✅ Model weights and vectorizer saved successfully!\")\n",
    "\n",
    "# --- 11. Deployment/Test Load (always use this in deployment) ---\n",
    "class TextEmbedder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128):\n",
    "        super(TextEmbedder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "input_dim = vectorizer.transform(['example']).shape[1]\n",
    "model = TextEmbedder(input_dim=input_dim, embed_dim=128)\n",
    "model.load_state_dict(torch.load(\"leetcode_model.pt\", map_location=torch.device('cpu')))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf9f4e-a66e-4b79-9367-c2201fcf012b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
